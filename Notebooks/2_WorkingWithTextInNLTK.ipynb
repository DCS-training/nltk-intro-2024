{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdd705aa",
   "metadata": {},
   "source": [
    "# NLTK with Lewis Grassic Gibbon First Editions\n",
    "\n",
    "**Data Source:** [National Library of Scotland Data Foundry](https://data.nls.uk/data/digitised-collections/lewis-grassic-gibbon-first-editions/)\n",
    "\n",
    "**Code Reference:** \n",
    "National Library of Scotland. Exploring Lewis Grassic Gibbon First Editions. National Library of Scotland, 2020. https://doi.org/10.34812/gq6w-6e91\n",
    "\n",
    "**Date:** April 6, 2022\n",
    "\n",
    "**Course:** Text Analysis with NLTK (Week 2, Class 3); Centre for Data, Culture & Society"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3121b01a",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Corpus Name:** Lewis Grassic Gibbon First Editions\n",
    "\n",
    "**Questions:**\n",
    "1. What are the most common words in the corpus?\n",
    "    * How many words are in the entire corpus?\n",
    "        * Create a list of all the alphabetic tokens (words)\n",
    "    * Calculate the frequency distribution (`FreqDist()`)\n",
    "    \n",
    "\n",
    "2. What are the most common words in one book from the corpus?\n",
    "    * Identify which files in the corpus are for which book\n",
    "        * Create a list of all the alphabetic tokens (words)\n",
    "    * Calculate the frequency distribution for individual files\n",
    "    \n",
    "\n",
    "3. How does the word choice of the author change from one book to another?\n",
    "    * How many words are in each book (each file in the corpus)?\n",
    "    * How many *unique* words are in each book?\n",
    "        * Normalize (standardize) the words by casefolding\n",
    "    * Lexical diversity = count of unique words / count of all words\n",
    "\n",
    "***\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "I. [Preparation](#preparation)\n",
    "\n",
    "II. [Normalization](#normalization)\n",
    "\n",
    "III. [Data Cleaning](#data_cleaning)\n",
    "\n",
    "IV. [Analysis](#analysis)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637cdd3f",
   "metadata": {},
   "source": [
    "<a id=\"preparation\"></a>\n",
    "## I. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c24f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install altair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52a5768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load a CSV file with an inventory of the documents in the corpus\n",
    "import pandas as pd\n",
    "\n",
    "# To create data visualizations\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To perform text analysis\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.text import Text\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.draw.dispersion import dispersion_plot as displt\n",
    "\n",
    "import re # Regular Expressions (RegEx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4923086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"nls-text-gibbon\"\n",
    "wordlists = PlaintextCorpusReader(data_directory, \"\\d.*\", encoding=\"latin1\")\n",
    "corpus_tokens = wordlists.words()  # method for tokenization\n",
    "print(corpus_tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e75de13",
   "metadata": {},
   "source": [
    "To get a sense of the data we're working with, let's create a functions that tell us how many tokens, sentences, and files are in our corpus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4530927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_tokens(plaintext_corpus_read_lists):\n",
    "    total_tokens = 0\n",
    "    for fileid in plaintext_corpus_read_lists.fileids():\n",
    "        total_tokens += len(plaintext_corpus_read_lists.words(fileid))\n",
    "    return total_tokens\n",
    "\n",
    "def calculate_total_sents(plaintext_corpus_read_lists):\n",
    "    total_sents = 0\n",
    "    for fileid in plaintext_corpus_read_lists.fileids():\n",
    "        total_sents += len(plaintext_corpus_read_lists.sents(fileid))\n",
    "    return total_sents\n",
    "\n",
    "def calculate_total_files(plaintext_corpus_read_lists):\n",
    "    return len(plaintext_corpus_read_lists.fileids())\n",
    "\n",
    "def display_corpus_statistics(name, total_tokens, total_sents, total_files):\n",
    "    print(f\"Total words in {name}: {total_tokens}\")\n",
    "    print(f\"Total sentences in {name}: {total_sents}\")\n",
    "    print(f\"Total files in {name}: {total_files}\")\n",
    "\n",
    "display_corpus_statistics(\n",
    "    \"Lewis Grassic Gibbon Corpus\",\n",
    "    calculate_total_tokens(wordlists),\n",
    "    calculate_total_sents(wordlists),\n",
    "    calculate_total_files(wordlists)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98374056",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(wordlists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadbfaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wordlists.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97a281e",
   "metadata": {},
   "source": [
    "There is also a csv file which gives some metadata. Let's load this into a pandas DataFrame with `pd.read_csv`. It has no header row, so set the `header` parameter to `None` and use the `names` parameter to give it the column names `'fileid'` and `'title'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d421466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee14061",
   "metadata": {},
   "source": [
    "Let's create a **dictionary**, one of the Python data types, that associates each `fileid` with each `title`.  That way we can quickly determine from our text analysis with NLTK what book we are looking at, since NLTK uses the fileids (the names of the files in our data directory, a.k.a. folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a825c427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain a list of all file IDs\n",
    "fileids = \"your code here\"\n",
    "print(f\"List of file IDs:\\n{fileids}\\n\")\n",
    "\n",
    "# obtain a list of all titles\n",
    "titles = \"your code here\"\n",
    "print(f\"List of titles:\\n{titles}\\n\")\n",
    "\n",
    "# create a dictionary where the keys are file IDs and the values are titles\n",
    "lgg_dict = \"your code here\"\n",
    "print(f\"Dictionary of file IDs and titles:\\n{lgg_dict}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8617570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can say...\n",
    "a_file_id = fileids[10]\n",
    "lgg_dict[a_file_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646bec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...or simply...\n",
    "lgg_dict['205174251.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f35b5dd",
   "metadata": {},
   "source": [
    "Let's create lists of all the words (alphabetic tokens) and sentences in the LGG corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa52f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_words_sents (plaintext_corpus_read_lists):\n",
    "    \"\"\"Creates lists of all words and all tokens in a given corpus.\n",
    "    I've added this docstring because docstrings are good practice,\n",
    "    and I can show you one of the standard formats for Python \n",
    "    docstrings\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        plaintext_corpus_read_lists (PlaintextCorpusReader):\n",
    "            corpus generated from a collection of plantext files\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        all_words (list):\n",
    "            list of all words: that is, all tokens consisting of alphabetical\n",
    "            characters only\n",
    "        all_sents (list)\n",
    "            list of all sentences\n",
    "    \"\"\"\n",
    "    pass # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d43cdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgg_words, lgg_sents = get_words_sents(wordlists)\n",
    "print(lgg_words[:20])\n",
    "print(lgg_sents[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109d0e4e",
   "metadata": {},
   "source": [
    "<a id=\"data_cleaning\"></a>\n",
    "## II. Data Cleaning with RegEx and NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f09919",
   "metadata": {},
   "source": [
    "=============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42680cb2",
   "metadata": {},
   "source": [
    "Let's calculate the frequency distribution, or the count of occurrences of each word across the entire corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c027e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_lgg = FreqDist(lgg_words)\n",
    "print(\"Total words after filtering:\", fdist_lgg.N())\n",
    "print(\"50 most common words after filtering:\", fdist_lgg.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed032bd1",
   "metadata": {},
   "source": [
    "Uh-oh! Note the 'â' and the '\\x80\\x99' - those don't look like they're meant to be there! The NLS Gibbon corpus is, after all, the result of scanning and running OCR (Optical Character Recognition), which is not totally reliable. Before doing our analysis, let's do some data cleaning, with ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-latex",
   "metadata": {},
   "source": [
    "### Regular Expresssions (RegEx)\n",
    "\n",
    "* **WHAT? Pattern matching strings in Python**\n",
    "* **WHY? To find specific words or phrases, or variations of a particular word or phrase**\n",
    "    * Once found, they can be replaced, so this is useful for cleaning text with digitization errors.  Optical Character Recognition (OCR) and Handwriting Recognition (HWT or HRT) technologies are imperfect, so you will find errors in digitized text corpora (unless of course they've been manually reviewed and corrected).\n",
    "* **HOW? Combinations of special characters with a RegEx compiler**\n",
    "    * In programming, a *compiler* translates code from one programming language to another.  In a sense, RegEx is a language that can sit on top of Python.  RegEx works with Python data types and syntax but it also has its own special characters and methods that plain Python doesn't use.\n",
    "    \n",
    "My favorite resource for practice with and testing Regular Expressions is  is [Regex101.com](https://regex101.com): also check out [Pythex.org](https://pythex.org) for the cheat sheet it provides!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To use Regular Expressions (RegEx)\n",
    "# import re\n",
    "\n",
    "# # To perform text analysis\n",
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# nltk.download('punkt')\n",
    "# from nltk.corpus import PlaintextCorpusReader\n",
    "# nltk.download('wordnet')\n",
    "# from nltk.corpus import wordnet\n",
    "# nltk.download('stopwords')\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.text import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-element",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgg = nltk.Text(lgg_words)\n",
    "print(lgg[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-defensive",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgg.concordance(\"\\x80\\x98\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3a0691",
   "metadata": {},
   "source": [
    "Looks like \"\\x80\\x98\" is the [UTF-8](https://en.wikipedia.org/wiki/UTF-8) encoding for the em-dash character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgg.concordance(\"â\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-significance",
   "metadata": {},
   "source": [
    "In addition to the â's, we can also see above some oddities with 'Â' - examples being:\n",
    "\n",
    "- `'himÂ ¬ self'`\n",
    "- `'proÂ ¬ tested'`\n",
    "- `'affirÂ ¬ mation'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-pillow",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgg.concordance(\"¬\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-snowboard",
   "metadata": {},
   "source": [
    "To remove a substring (a selection of characters in a string), we can use an empty string (either `\"\"` or `''`) as the second input for the `replace()` method.  Just remember to set the Text object followed by this method to a variable, otherwise your changes won't be saved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .replace() must be used on a string object, not a Text object\n",
    "lgg_str = wordlists.raw()\n",
    "# your code here\n",
    "# _________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-throw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .concordance() must be used on a Text object, not a string object\n",
    "corpus_tokens = word_tokenize(lgg_str)\n",
    "lgg = nltk.Text(corpus_tokens)\n",
    "lgg.concordance(\"¬\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-primary",
   "metadata": {},
   "source": [
    "It worked!\n",
    "\n",
    "Let's try using RegEx to clean the text now. Write a regular expression which finds seqences consisting of one or more alphabetic characters, followed by 'â' or 'Â'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"your code here\"\n",
    "re_pattern = re.compile(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_errors = re_pattern.findall(lgg_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-avenue",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(digit_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_errors = list(set(digit_errors))\n",
    "len(unique_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-airplane",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_errors[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-closing",
   "metadata": {},
   "source": [
    "Since the `â` character keeps appearing at the end of tokens, let's first re-tokenise `lgg_str`, then create use `strip()` to remove them for practice with that method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, re-tokenise\n",
    "lgg_tokens = word_tokenize(lgg_str)\n",
    "print(lgg_tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae620e",
   "metadata": {},
   "source": [
    "Let's make a function that strips 'â' from our tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-ebony",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Let's try it first with a sample subcorpus\n",
    "subcorpus = lgg_tokens[:1000]\n",
    "\n",
    "def strip_in_corpus(corpus, characters):\n",
    "    pass # your code here\n",
    "\n",
    "clean_subcorpus = strip_in_corpus(subcorpus, 'âÂ')\n",
    "print(clean_subcorpus[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0518f6c",
   "metadata": {},
   "source": [
    "I've made a function to check exactly what is being changed in the subcorpus. Has this solved our â-problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tokens(tokens1, tokens2):\n",
    "    diffs = {}\n",
    "    for i, pair in enumerate(zip(tokens1, tokens2)):\n",
    "        a, b = pair\n",
    "        if a!=b:\n",
    "            diffs[i] = (a, b)\n",
    "    return diffs\n",
    "\n",
    "print(compare_tokens(subcorpus, clean_subcorpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca001d0",
   "metadata": {},
   "source": [
    "It would appear not. OK, let's try making a function to show us all the tokens that contain the pattern matched by the regular expression we created earlier, `re_pattern`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518a91cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_find_pattern(corpus, pattern):\n",
    "    pass # your code here\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be451e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(corpus_find_pattern(lgg_tokens, re_pattern))[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ab21be",
   "metadata": {},
   "source": [
    "Aaaaaand this was the point at which I figured out what is going on with these weird sequences. \n",
    "\n",
    "I did a bit of detective work on it, and it turns out these sequences are UTF-8 characters, incorrectly decoded in 'latin1', the character encoding we used when we first read the data from the directory. For instance, in UTF-8 an open double curly quote `“` is encoded as `\\xe2\\x80\\x9b`. This encoding uses three binary bytes to encode the character. The `\\x` is an escape sequence that indicates the following two characters are a binary byte written in hexidecimal - that is, base 16. So, `e2` is 226, which in latin1 is 'â'. `\\x80\\x9b` isn't anything in latin1, so it isn't decoded as anything.\n",
    "\n",
    "The sensible thing to do here, the thing that we would in fact do if this was a real project, rather than an exercise for a class, would be this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1034d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wordlists_utf8 = PlaintextCorpusReader(data_directory, \"\\d.*\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cc641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wordlists_utf8.words()[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe3907c",
   "metadata": {},
   "source": [
    "...And then we'd re-do the data processing we've done so far, and re-check it for any weird artefacts and OCR-errors. However, there is another point I'd like to make about this sort of data analysis, which is that, depending on what it is you are trying to find out, sometimes a time-consuming and painstaking data-cleaning isn't needed. If we want to do an analysis of Gibbon's lexicon, for instance, we might just toss out every malformed token, and do our analysis using the rest.\n",
    "\n",
    "So, let's make a function that goes through a corpus and removes every token that contains 'â', 'Â', or any non-alphabetic character: (note that `str.isapha()` considers 'â' and 'Â' to be alphabetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1becca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_nonalphabetic(corpus):\n",
    "    pass # your code here\n",
    "    \n",
    "lgg_alpha = filter_nonalphabetic(lgg_tokens)\n",
    "print(lgg_alpha[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4abb94",
   "metadata": {},
   "source": [
    "There. Good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-equation",
   "metadata": {},
   "source": [
    "Ta da!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5db2675",
   "metadata": {},
   "source": [
    "<a id=\"normalization\"></a>\n",
    "## III. Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd443fb",
   "metadata": {},
   "source": [
    "A bit more preprocessing before we start our analysis.\n",
    "\n",
    "Let's casefold to normalize so capitalized and lowercased versions of words are considered the same word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b7cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_all(words):\n",
    "    pass # your code here\n",
    "\n",
    "lgg_words_lower = lowercase_all(lgg_alpha)\n",
    "print(lgg_words_lower[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75409713",
   "metadata": {},
   "source": [
    "...and exclude stopwords using `stopwords.words(language)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1d4f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_stopwords(corpus: list[str], min_len: int, language: str) -> list[str]:\n",
    "    \"\"\"Iterates through a list of words and removes all words of length less\n",
    "    than min_len, and all words in `stopwords.words(language)`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        words (list of str):\n",
    "            the list of words to be filtered \n",
    "        min_len (int):\n",
    "            minimum length for words: words shorter than this should be \n",
    "            filtered out\n",
    "        language (str):\n",
    "            name of corpus language, in lower case. This is used to identify\n",
    "            the correct stopwords list\n",
    "    \"\"\"\n",
    "    pass # your code here\n",
    "\n",
    "filtered_lower = remove_stopwords(lgg_words_lower, 3, 'english')\n",
    "print(len(lgg_words_lower))\n",
    "print(len(filtered_lower))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c0a32f",
   "metadata": {},
   "source": [
    "Before we go on to analysis, here's a function that chains together all the processing we've done, in case we need to do it again for any reason..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85220beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_corpus(wordlist, min_len=3, language='english'):\n",
    "    processed = filter_nonalphabetic(wordlist)\n",
    "    processed = lowercase_all(processed)\n",
    "    return remove_stopwords(processed, min_len, language)\n",
    "\n",
    "wordlists2 = PlaintextCorpusReader(data_directory, \"\\d.*\", encoding=\"latin1\")\n",
    "corpus_tokens2, _ = get_words_sents(wordlists2)\n",
    "processed_corpus = process_corpus(corpus_tokens2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675971ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed_corpus[60:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1cc781",
   "metadata": {},
   "source": [
    "<a id=\"analysis\"></a>\n",
    "## IV. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63226db",
   "metadata": {},
   "source": [
    "Let's calculate the frequency distribution again, now that we have cleaned and normalised our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1e653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_filtered_lower = FreqDist(processed_corpus)\n",
    "print(\"Total words after filtering:\", fdist_filtered_lower.N())\n",
    "print(\"50 most common words after filtering:\", fdist_filtered_lower.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa06f33",
   "metadata": {},
   "source": [
    "Happily, I don't see anything there that looks dubious.\n",
    "\n",
    "Let's plot the frequency distributions of the *n* most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5b6d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_plot(corpus, n):\n",
    "    fdist_filtered_lower = FreqDist(filtered_lower)\n",
    "    plt.figure(figsize = (20, 8))\n",
    "    plt.rc('font', size=12)\n",
    "    fdist_filtered_lower.plot(n, title=f'Frequency Distribution for {n} Most Common Tokens in the Standardised LGG Dataset (excluding stop words)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6e87af",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_plot(processed_corpus, 20) # Try increasing or decreasing this number to view more or fewer tokens in the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9727f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT: wordlists and the fileid of the wordlist to be tokenised\n",
    "# OUTPUT: a list of word tokens (in String format) for the inputted fileid\n",
    "def get_words(plaintext_corpus_read_lists, fileid):\n",
    "    file_words = process_corpus(plaintext_corpus_read_lists.words(fileid))\n",
    "    str_words = [str(word) for word in file_words]    \n",
    "    return str_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff6e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(fileids):\n",
    "    words_by_file = []\n",
    "    for file in fileids:\n",
    "        words_by_file += [get_words(wordlists, file)]\n",
    "    return words_by_file\n",
    "\n",
    "words_by_file = get_all_words(fileids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6837450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT: a list of words in String format\n",
    "# OUTPUT: the number of unique words divided by\n",
    "#         the total words in the inputted list\n",
    "def lexical_diversity(str_words_list):\n",
    "    return len(set(str_words_list))/len(str_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e3fcca",
   "metadata": {},
   "source": [
    "Add to the inventory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21cd1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexdiv_by_file = []\n",
    "for words in words_by_file:\n",
    "    lexdiv_by_file += [lexical_diversity(words)]\n",
    "\n",
    "df['lexicaldiversity'] = lexdiv_by_file\n",
    "df_lexdiv = df.sort_values(by=['lexicaldiversity', 'title'], inplace=False, ascending=True)\n",
    "df_lexdiv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83080b2",
   "metadata": {},
   "source": [
    "For the entire corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c06524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_diversity(processed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7743dd02",
   "metadata": {},
   "source": [
    "The table isn't bad but charts can make it easier to compare calculations more quickly, so let's visualize the lexical diversity scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf21f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_titles = list(df_lexdiv['title'])\n",
    "sorted_lexdiv = list(df_lexdiv['lexicaldiversity'])\n",
    "source = pd.DataFrame({\n",
    "    'Title': sorted_titles,\n",
    "    'Lexdiv': sorted_lexdiv\n",
    "})\n",
    "\n",
    "alt.Chart(source, title=\"Lexical Diversity of Gibbon's Works\").mark_bar(size=30).encode(\n",
    "    alt.X('Title', axis=alt.Axis(title='Lewis Grassic Gibbon Work'), type='nominal', sort=None),  # If sort unspecified, chart will sort x-axis values alphabetically\n",
    "    alt.Y('Lexdiv', axis=alt.Axis(title='Lexical Diversity')),\n",
    "    alt.Order(\n",
    "      # Sort the segments of the bars by this field\n",
    "      'Lexdiv',\n",
    "      sort='ascending'\n",
    "    )\n",
    ").configure_axis(\n",
    "    grid=False\n",
    ").configure_view(\n",
    "    strokeWidth=0\n",
    ").properties(\n",
    "    width=500\n",
    ")\n",
    "\n",
    "#     alt.Y('Lexdiv', axis=alt.Axis(format='%', title='Lexical Diversity')),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe8e0d1",
   "metadata": {},
   "source": [
    "Could we sort these chronologically?\n",
    "\n",
    "We can add information available from the [digital.nls.uk](https://data.nls.uk/wp-content/uploads/2020/10/digital.nls.uk) website about the publication dates to our inventory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d0eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "published = [1932, 1933, 1933, 1934, 1933, 1932, 1934, 1934, 1934, 1931, 1932, 1934, 1932, 1930, 1931, 1928]\n",
    "df_lexdiv['published'] = published\n",
    "df_pub = df_lexdiv.sort_values(by=['published', 'title'], inplace=False, ascending=True)\n",
    "df_pub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf74063",
   "metadata": {},
   "source": [
    "Then we can recreate the bar chart with the bars (books) sorted by year of publication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef7c7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_titles = list(df_pub['title'])\n",
    "sorted_lexdiv = list(df_pub['lexicaldiversity'])\n",
    "sorted_published = list(df_pub['published'])\n",
    "source = pd.DataFrame({\n",
    "    'Title': sorted_titles,\n",
    "    'Lexdiv': sorted_lexdiv,\n",
    "    'Published': sorted_published\n",
    "})\n",
    "\n",
    "alt.Chart(source, title=\"Lexical Diversity of Gibbon's Works\").mark_bar(size=30).encode(\n",
    "    alt.X('Title', axis=alt.Axis(title='Title of Lewis Grassic Gibbon Work'), type='nominal', sort=None),  # If sort unspecified, chart will sort x-axis values alphabetically\n",
    "    alt.Y('Lexdiv', axis=alt.Axis(title='Lexical Diversity')),\n",
    "    alt.Order(\n",
    "      # Sort the segments of the bars by this field\n",
    "      'Lexdiv',\n",
    "      sort='descending'\n",
    "    ),\n",
    "    color=alt.Color('Published:O', legend = alt.Legend(title='Date Published')),\n",
    "    tooltip='Title:N'\n",
    ").configure_axis(\n",
    "    grid=False,\n",
    "    labelFontSize=12,\n",
    "    titleFontSize=12,\n",
    "    labelAngle=-45\n",
    ").configure_title(\n",
    "    fontSize=14,\n",
    ").configure_view(\n",
    "    strokeWidth=0\n",
    ").properties(\n",
    "    width=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6444c99e",
   "metadata": {},
   "source": [
    "To get the lexical diversity per year..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5692db78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary associating works with year they were published\n",
    "pub_yr = {1928: [], 1930: [], 1931: [], 1932: [], 1933: [], 1934: []}\n",
    "for index,row in df_pub.iterrows():\n",
    "    pub_yr[row[3]] += [row[0]]\n",
    "print(pub_yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8148191",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexdiv_by_year = []\n",
    "for key,value in pub_yr.items():\n",
    "    lexdiv_by_file = []\n",
    "    for fileid in value:\n",
    "        file_words = wordlists.words(fileid)\n",
    "        str_words = [str(w.lower()) for w in file_words if w.isalpha()]\n",
    "        lexdiv_by_file += [lexical_diversity(str_words)]\n",
    "    lexdiv_by_year += [sum(lexdiv_by_file)/len(lexdiv_by_file)]\n",
    "print(lexdiv_by_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39098fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_years = [1928, 1930, 1931, 1932, 1933, 1934]\n",
    "pub_lex = dict(zip(pub_years, lexdiv_by_year))\n",
    "pub_lex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d4f744",
   "metadata": {},
   "source": [
    "Now we can visualize the average lexical diversity score for each year Gibbon published in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1597cb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "source = pd.DataFrame({\n",
    "    'Year': pub_years,\n",
    "    'Average Lexical Diversity': lexdiv_by_year\n",
    "})\n",
    "\n",
    "alt.Chart(source, title=\"Average Yearly Lexical Diversity of Gibbon First Editions\").mark_bar(size=60).encode(\n",
    "    alt.X('Year', axis=alt.Axis(title='Year of Publication'), type='ordinal'),\n",
    "    alt.Y('Average Lexical Diversity', axis=alt.Axis(title='Average Lexical Diversity'))\n",
    ").configure_axis(\n",
    "    grid=False,\n",
    "    labelFontSize=12,\n",
    "    titleFontSize=12,\n",
    "    labelAngle=0\n",
    ").configure_title(\n",
    "    fontSize=14,\n",
    ").configure_view(\n",
    "    strokeWidth=0\n",
    ").properties(\n",
    "    width=365\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e9e5ae",
   "metadata": {},
   "source": [
    "So Gibbon's lexical diversity does decrease over time, excepting a small increase in the last year he published, 1934!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
